{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1WwWBsMorUqge6jEd8VafRdRulASIvVZu",
      "authorship_tag": "ABX9TyM1ezTZD5RHhUbXAzCHOJY4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthewshawnkehoe/Data-Analysis/blob/main/fire_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load images and inspect data\n",
        "\n"
      ],
      "metadata": {
        "id": "vbK6Cr605sLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of loading an image with the Keras API\n",
        "from tensorflow.keras.utils import load_img\n",
        "# load the image\n",
        "byteImgIO = io.BytesIO()\n",
        "byteImg = Image.open(\"/content/drive/MyDrive/data/train/BeaverBroadwaySnake_20160619.tif\")\n",
        "byteImg.save(byteImgIO, \"PNG\")\n",
        "byteImgIO.seek(0)\n",
        "img = byteImgIO.read()\n",
        "# img = load_img('/content/drive/MyDrive/data/train/BeaverBroadwaySnake_20160619.tif')\n",
        "# report details about the image\n",
        "print(type(img))\n",
        "print(img.format)\n",
        "print(img.mode)\n",
        "print(img.size)\n",
        "# show the image\n",
        "img.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "Ip7EthY8k4qW",
        "outputId": "bd967eac-cdf0-40f0-ce05-2946e855f5a5"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnidentifiedImageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-570f005c38eb>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# load the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbyteImgIO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbyteImg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/data/train/BeaverBroadwaySnake_20160619.tif\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mbyteImg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyteImgIO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PNG\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mbyteImgIO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3028\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maccept_warnings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3029\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3030\u001b[0;31m     raise UnidentifiedImageError(\n\u001b[0m\u001b[1;32m   3031\u001b[0m         \u001b[0;34m\"cannot identify image file %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3032\u001b[0m     )\n",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file '/content/drive/MyDrive/data/train/BeaverBroadwaySnake_20160619.tif'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Progressively Load Images"
      ],
      "metadata": {
        "id": "VGq9hps3kubJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "t1IhBgJ0ksQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is possible to write code to manually load image data and return data ready for modeling.\n",
        "\n",
        "This would include walking the directory structure for a dataset, loading image data, and returning the input (pixel arrays) and output (class integer).\n",
        "\n",
        "Thankfully, we don't need to write this code. Instead, we can use the [ImageDataGenerator class](https://keras.io/preprocessing/image/) provided by Keras.\n",
        "\n",
        "The main benefit of using this class to load the data is that images are loaded for a single dataset in batches, meaning that it can be used for loading both small datasets as well as very large image datasets with thousands or millions of images.\n",
        "\n",
        "Instead of loading all images into memory, it will load just enough images into memory for the current and perhaps the next few mini-batches when training and evaluating a deep learning model. I refer to this as progressive loading, as the dataset is progressively loaded fromz file, retrieving just enough data for what is needed immediately.\n",
        "\n",
        "Two additional benefits of the using the *ImageDataGenerator* class is that it can also automatically scale pixel values of images and it can automatically generate augmented versions of images. We will leave these topics for discussion in another tutorial and instead focus on how to use the *ImageDataGenerator* class to load image data from file."
      ],
      "metadata": {
        "id": "hqATwY499Nlq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pattern for using the ImageDataGenerator class is used as follows:\n",
        "\n",
        "1. Construct and configure an instance of the *ImageDataGenerator* class.\n",
        "2. Retrieve an iterator by calling the `flow_from_directory()` function.\n",
        "3. Use the iterator in the training or evaluation of a model.\n",
        "\n"
      ],
      "metadata": {
        "id": "7p7g5wYD5voJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s take a closer look at each step.\n",
        "\n",
        "The constructor for the *ImageDataGenerator* contains many arguments to specify how to manipulate the image data after it is loaded, including pixel scaling and [data augmentation](https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/). We do not need any of these features at this stage, so configuring the *ImageDataGenerator* is easy."
      ],
      "metadata": {
        "id": "5-tA-6cp6xqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# create a data generator\n",
        "datagen = ImageDataGenerator()"
      ],
      "metadata": {
        "id": "gVpVkPDX6Fei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, an iterator is required to progressively load images for a single dataset.\n",
        "\n",
        "This requires calling the `flow_from_directory()` function and specifying the dataset directory, such as the train, test, or validation directory.\n",
        "\n",
        "The function also allows you to configure more details related to the loading of images. Of note is the `target_size` argument that allows you to load all images to a specific size, which is often required when modeling. The function defaults to square images with the size (256, 256).\n",
        "\n",
        "The function also allows you to specify the type of classification task via the `class_mode` argument, specifically whether it is `binary` or a multi-class classification `categorical`.\n",
        "\n",
        "The default `batch_size` is 32, which means that 32 randomly selected images from across the classes in the dataset will be returned in each batch when training. Larger or smaller batches may be desired. You may also want to return batches in a deterministic order when evaluating a model, which you can do by setting `shuffle` to `False.`\n",
        "\n",
        "There are many other options, and I encourage you to review the [API documentation](https://keras.io/preprocessing/image/).\n",
        "\n",
        "We can use the same *ImageDataGenerator* to prepare separate iterators for separate dataset directories. This is useful if we would like the same pixel scaling applied to multiple datasets (e.g. train, test, etc.)."
      ],
      "metadata": {
        "id": "GrPW_ggx65gR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip a local copy of the test, validate, and train files\n"
      ],
      "metadata": {
        "id": "qyUvxTlNCKXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_it = datagen.flow_from_directory(\n",
        "#     '/content/drive/MyDrive/data/train',\n",
        "#     # class_mode='binary',\n",
        "#     classes=\"images\",\n",
        "#     batch_size=16,\n",
        "#     color_mode=\"grayscale\",\n",
        "#     target_size=(400, 400),\n",
        "#     seed=100,\n",
        "#     data_format='tiff',\n",
        "#     shuffle=True)\n",
        "\n",
        "# load and iterate training dataset\n",
        "train_it = datagen.flow_from_directory('/content/drive/MyDrive/data/', classes=['train'], class_mode='categorical', batch_size=32)\n",
        "# load and iterate validation dataset\n",
        "val_it = datagen.flow_from_directory('/content/drive/MyDrive/data/', classes=['validation'], class_mode='categorical', batch_size=32)\n",
        "# load and iterate test dataset\n",
        "test_it = datagen.flow_from_directory('/content/drive/MyDrive/data/', classes=['test'], class_mode='categorical', batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAFcM4eO6_ix",
        "outputId": "89f967ea-e251-49a6-faea-f9025bf1cc79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 172 images belonging to 1 classes.\n",
            "Found 86 images belonging to 1 classes.\n",
            "Found 86 images belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the iterators have been prepared, we can use them when fitting and evaluating a deep learning model.\n",
        "\n",
        "For example, fitting a model with a data generator can be achieved by calling the `fit_generator()` function on the model and passing the training iterator (`train_it`). The validation iterator (`val_it`) can be specified when calling this function via the `validation_data` argument.\n",
        "\n",
        "The `steps_per_epoch` argument must be specified for the training iterator in order to define how many batches of images defines a single epoch.\n",
        "\n",
        "For example, if you have 1,000 images in the training dataset (across all classes) and a batch size of 64, then the `steps_per_epoch` would be about 16, or 1000/64.\n",
        "\n",
        "Similarly, if a validation iterator is applied, then the `validation_steps` argument must also be specified to indicate the number of batches in the validation dataset defining one epoch."
      ],
      "metadata": {
        "id": "lU7fEVnc7m61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "model = ...\n",
        "# fit model\n",
        "model.fit_generator(train_it, steps_per_epoch=16, validation_data=val_it, validation_steps=8)"
      ],
      "metadata": {
        "id": "ibbZrVYT7wg5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "d075a12b-a0a6-4b7f-a4de-97c3664c3c5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-4f2b60229b8c>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_it\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_it\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'fit_generator'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model is fit, it can be evaluated on a test dataset using the `evaluate_generator()` function and passing in the test iterator (`test_it`). The `steps` argument defines the number of batches of samples to step through when evaluating the model before stopping."
      ],
      "metadata": {
        "id": "ERhcBd8y7709"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model\n",
        "loss = model.evaluate_generator(test_it, steps=24)"
      ],
      "metadata": {
        "id": "LylHrf658CJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, if you want to use your fit model for making predictions on a very large dataset, you can create an iterator for that dataset as well (e.g. `predict_it`) and call the `predict_generator()` function on the model."
      ],
      "metadata": {
        "id": "Uv64M2Vz8GV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make a prediction\n",
        "yhat = model.predict_generator(predict_it, steps=24)"
      ],
      "metadata": {
        "id": "PefLCHwn8OvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use our small dataset defined in the previous section to demonstrate how to define an *ImageDataGenerator* instance and prepare the dataset iterators.\n",
        "\n",
        "A complete example is listed below."
      ],
      "metadata": {
        "id": "7cdseWVK8Ree"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "from io import BytesIO\n",
        "from PIL import Image, ImageFile\n",
        "import numpy\n",
        "\n",
        "# example of progressively loading images from file\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# create generator\n",
        "datagen = ImageDataGenerator()\n",
        "# prepare an iterators for each dataset\n",
        "train_it = datagen.flow_from_directory('/content/drive/MyDrive/data/', classes=['train_jpeg'], class_mode='categorical', batch_size=32)\n",
        "val_it = datagen.flow_from_directory('/content/drive/MyDrive/data/', classes=['validation_jpeg'], class_mode='categorical', batch_size=32)\n",
        "test_it = datagen.flow_from_directory('/content/drive/MyDrive/data/', classes=['test_jpeg'], class_mode='categorical', batch_size=32)\n",
        "\n",
        "# Correct the image format\n",
        "print(type(train_it))\n",
        "\n",
        "batchX, batchy = train_it.next()\n",
        "print('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))"
      ],
      "metadata": {
        "id": "N8WvSBu88UQZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79266727-1769-46d4-f86e-6a720c7480c0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 172 images belonging to 1 classes.\n",
            "Found 86 images belonging to 1 classes.\n",
            "Found 86 images belonging to 1 classes.\n",
            "<class 'keras.preprocessing.image.DirectoryIterator'>\n",
            "Batch shape=(32, 256, 256, 3), min=0.000, max=255.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the example first creates an instance of the *ImageDataGenerator* with all default configuration.\n",
        "\n",
        "Next, three iterators are created, one for each of the train, validation, and test binary classification datasets. As each iterator is created, we can see debug messages reporting the number of images and classes discovered and prepared.\n",
        "\n",
        "Finally, we test out the train iterator that would be used to fit a model. The first batch of images is retrieved and we can confirm that the batch contains two images, as only two images were available. We can also confirm that the images were loaded and forced to the square dimensions of 256 rows and 256 columns of pixels and the pixel data was not scaled and remains in the range [0, 255]."
      ],
      "metadata": {
        "id": "Zh3SoR6W8dIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Toy Convnet"
      ],
      "metadata": {
        "id": "iWnXv6CI4J80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a small convnet\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "TS_ryLMxdN_E"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the model’s summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kC3ImdyLb-JM",
        "outputId": "2e3c1099-693c-4ecb-ab5d-0b4466e01209"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 3, 3, 128)         73856     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1152)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                11530     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 104,202\n",
            "Trainable params: 104,202\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the convnet on MNIST images\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype(\"float32\") / 255\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "loss=\"sparse_categorical_crossentropy\",\n",
        "metrics=[\"accuracy\"])\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0Oy6W-zcFXo",
        "outputId": "3aca2adb-fb00-4dec-d8dc-3e5d3e2bf2f5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n",
            "Epoch 1/5\n",
            "938/938 [==============================] - 17s 5ms/step - loss: 0.1548 - accuracy: 0.9520\n",
            "Epoch 2/5\n",
            "938/938 [==============================] - 3s 4ms/step - loss: 0.0440 - accuracy: 0.9864\n",
            "Epoch 3/5\n",
            "938/938 [==============================] - 3s 4ms/step - loss: 0.0298 - accuracy: 0.9905\n",
            "Epoch 4/5\n",
            "938/938 [==============================] - 4s 5ms/step - loss: 0.0229 - accuracy: 0.9931\n",
            "Epoch 5/5\n",
            "938/938 [==============================] - 4s 4ms/step - loss: 0.0176 - accuracy: 0.9946\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1e1c0d3a30>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the convnet on test data\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "FthZ6Dx34bQL",
        "outputId": "fab54a59-acb0-4666-905f-a00f322e50ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0285 - accuracy: 0.9916\n",
            "Test accuracy: 0.992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your convnet"
      ],
      "metadata": {
        "id": "nJ4aD9P34pPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a convnet matching the shape of your data\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "inputs = keras.Input(shape=(256, 256, 1))\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "0lGJYn6v4qm6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}