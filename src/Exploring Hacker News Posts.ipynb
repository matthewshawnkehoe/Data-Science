{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exploring Hacker News Posts \n",
    "\n",
    "Time is a crucial factor in any analysis. From the analysis of time, we can obtain clearer insights into underlying trends and systemic patterns. For example, Information from time analysis could help a social media user in selecting the best time post content that maximally engages the community.\n",
    "\n",
    "In this project, we will work with a dataset of submissions to a popular technology site called Hacker News. We will try to identify the 'golden hours' for users looking to create posts on the platform, then we will explore the type of posts that draw user engagement.\n",
    "\n",
    "![](https://wiredcraft.com/images/posts/hacker-news.png)\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "[Hacker News](https://news.ycombinator.com/) is a site started by the startup incubator, [Y Combinator](https://www.ycombinator.com/), where user-submitted stories (known as \"posts\") receive votes and comments, similar to reddit. The platform is extremely popular in technology and startup circles, and posts that make it to the top listing can get hundreds of thousands of visitors. \n",
    "\n",
    "\n",
    "## Dataset Information\n",
    "\n",
    "The original dataset can be found [here](https://www.kaggle.com/hacker-news/hacker-news-posts). For this analysis, we have reduced the data from almost *300,000 rows* to approximately *20,000 rows*. We did this by eliminating submissions that didn't receive any comments and then randomly sampling from the remaining submissions. The `7` columns in the dataset are described below:\n",
    "\n",
    "- `id`:- Unique identifier from Hacker News for the post\n",
    "- `title`:- Title of the post\n",
    "- `url`:- The URL that the posts links to, if the post has a URL\n",
    "- `num_points`:- Number of points the post acquired (calculated as *total upvotes - total downvotes*)\n",
    "- `num_comments`:- Number of comments on the post\n",
    "- `author`:- Username of the person who submitted the post\n",
    "- `created_at`:- Date and time of the post's submission\n",
    "\n",
    "## Categories of Interest\n",
    "\n",
    "We're specifically interested in posts with titles that begin with either Ask HN or Show HN. \n",
    "\n",
    "- Users submit `Ask HN` posts to ask the Hacker News community a specific question, for example:\n",
    "\n",
    "```\n",
    "Ask HN: How to improve my personal website?\n",
    "Ask HN: Am I the only one outraged by Twitter shutting down share counts?\n",
    "\n",
    "```\n",
    "- Users submit `Show HN` posts to show the Hacker News community a project, product, or just something interesting. Below are a few examples:\n",
    "\n",
    "```\n",
    "Show HN: Wio Link  ESP8266 Based Web of Things Hardware Development Platform'\n",
    "Show HN: Something pointless I made\n",
    "\n",
    "```\n",
    "\n",
    "## Importing Libraries\n",
    "As we progress in this project, we will need to read files; work with date and time; and create visualizations. To make these processes possible, we will import python's `reader` and `datetime` modules, including plotly's visualisation libraries - `plotly.express`, `plotly.graph_objects` and `plotly.subplots`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "import datetime as dt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Opening and Exploration\n",
    "In our attempt to open and explore the dataset, we will create a function called `extract_data()` which takes a file path as an argument, then returns a read version of the file as a list of lists. For ease of analysis, the function will return the column headers and the actual data as seperate entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filepath):\n",
    "    \"\"\"locates file using filepath, reads it, then returns a list of lists\"\"\"\n",
    "    \n",
    "    opened_file = open(filepath)\n",
    "    read_file = reader(opened_file)\n",
    "    result = list(read_file)\n",
    "    \n",
    "    # return both the column headers and the actual data as seperate entities\n",
    "    return result[0], result[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define another function called `explore_data()` to enable us display specified ranges of dataset rows in readable format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_data(dataset, start, end, rows_and_columns=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Displays dataset rows in readable format\n",
    "  \n",
    "    Parameters:\n",
    "    dataset (list): a list of lists\n",
    "    start (int): start index for dataset slice\n",
    "    end (int): end index for dataset slice\n",
    "    rows and columns (boolean): specifies whether to print the number of rows and columns.\n",
    "    \n",
    "    output:\n",
    "    prints the sliced dataset rows in readable format\n",
    "    prints the number of dataset rows and columns if rows_and_columns is True\n",
    "  \n",
    "    \"\"\"\n",
    "    \n",
    "    dataset_slice = dataset[start:end]    \n",
    "    for row in dataset_slice:\n",
    "        print(row)\n",
    "        print('\\n')\n",
    "\n",
    "    if rows_and_columns:\n",
    "        print('Number of rows: {:,}'.format(len(dataset)))\n",
    "        print('Number of columns:', len(dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's open and explore the Hacker News dataset using the functions we created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at'] \n",
      "\n",
      "['12224879', 'Interactive Dynamic Video', 'http://www.interactivedynamicvideo.com/', '386', '52', 'ne0phyte', '8/4/2016 11:52']\n",
      "\n",
      "\n",
      "['10975351', 'How to Use Open Source and Shut the Fuck Up at the Same Time', 'http://hueniverse.com/2016/01/26/how-to-use-open-source-and-shut-the-fuck-up-at-the-same-time/', '39', '10', 'josep2', '1/26/2016 19:30']\n",
      "\n",
      "\n",
      "['11964716', \"Florida DJs May Face Felony for April Fools' Water Joke\", 'http://www.thewire.com/entertainment/2013/04/florida-djs-april-fools-water-joke/63798/', '2', '1', 'vezycash', '6/23/2016 22:20']\n",
      "\n",
      "\n",
      "['11919867', 'Technology ventures: From Idea to Enterprise', 'https://www.amazon.com/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429', '3', '1', 'hswarna', '6/17/2016 0:01']\n",
      "\n",
      "\n",
      "Number of rows: 20,100\n",
      "Number of columns: 7\n"
     ]
    }
   ],
   "source": [
    "# extract the header and data from 'hacker_news.csv'\n",
    "hn_header, hn = extract_data('hacker_news.csv')\n",
    "\n",
    "# print the column headers and explore the first five rows of 'hacker_news.csv'\n",
    "print(hn_header, '\\n')\n",
    "\n",
    "explore_data(hn, 0, 4, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "*The extracted data contains a total of **20,100 rows** and **7 columns**. The following columns will be useful in our analysis - `title`, `num_points`, `num_comments`, and `created_at`.*\n",
    "\n",
    "Now that we have extracted the dataset and identified the columns that are relevant to our analysis, we are ready to isolate the posts that are of particular interest to us.\n",
    "\n",
    "## Isolating the Ask HN and Show HN Posts\n",
    "\n",
    "We are only concerned with post titles beginning with `Ask HN` or `Show HN`. Thus, we will create new lists containing just the data for those titles and store them in corresponding variables.\n",
    "\n",
    "To find the posts that begin with either Ask HN or Show HN, we'll use Python's built-in string method `string.startswith()`. We also need the `string.lower_method()` to ensure that our search is case insensitive (i.e unaffected by irregular capitalizations of `Ask HN` or `Show HN`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Removing Headers from a List of Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the first list in the inner lists contains the column headers, and the lists after contain the data for one row. In order to analyze our data, we need to first remove the row containing the column headers. Let's remove that first row next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def open_dataset(file_name, header=True):        \n",
    "    opened_file = open(file_name, encoding='utf8')\n",
    "    read_file = reader(opened_file)\n",
    "    data = list(read_file)\n",
    "    \n",
    "    if header:\n",
    "        return data[1:], data[0]\n",
    "    else:\n",
    "        return data\n",
    "    \n",
    "hn, headers = open_dataset('hacker_news.csv')\n",
    "display(headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's display the first five rows of `hn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12224879', 'Interactive Dynamic Video', 'http://www.interactivedynamicvideo.com/', '386', '52', 'ne0phyte', '8/4/2016 11:52']\n",
      "['10975351', 'How to Use Open Source and Shut the Fuck Up at the Same Time', 'http://hueniverse.com/2016/01/26/how-to-use-open-source-and-shut-the-fuck-up-at-the-same-time/', '39', '10', 'josep2', '1/26/2016 19:30']\n",
      "['11964716', \"Florida DJs May Face Felony for April Fools' Water Joke\", 'http://www.thewire.com/entertainment/2013/04/florida-djs-april-fools-water-joke/63798/', '2', '1', 'vezycash', '6/23/2016 22:20']\n",
      "['11919867', 'Technology ventures: From Idea to Enterprise', 'https://www.amazon.com/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429', '3', '1', 'hswarna', '6/17/2016 0:01']\n",
      "['10301696', 'Note by Note: The Making of Steinway L1037 (2007)', 'http://www.nytimes.com/2007/11/07/movies/07stein.html?_r=0', '8', '2', 'walterbell', '9/30/2015 4:12']\n"
     ]
    }
   ],
   "source": [
    "for row in hn[:5]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracting Ask HN and Show HN Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've removed the headers from `hn`, we're ready to filter our data. Since we're only concerned with post titles beginning with `Ask HN` or `Show HN`, we'll create new lists of lists containing just the data for those titles. To find the posts that begin with either `Ask HN` or `Show HN`, we'll use the string method startswith.\n",
    "\n",
    "First, we will create three empty lists called `ask_posts`, `show_posts`, and `other_posts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_posts = []\n",
    "show_posts = []\n",
    "other_posts =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will loop through each row in `hn` and seperate all post titles into `ask_posts`, `show_posts`, and `other_posts`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1744\n",
      "1162\n",
      "17194\n"
     ]
    }
   ],
   "source": [
    "for row in hn:\n",
    "    title = row[1].lower()\n",
    "    if title.startswith('ask hn'):\n",
    "        ask_posts.append(row)\n",
    "    elif title.startswith('show hn'):\n",
    "        show_posts.append(row)\n",
    "    else:\n",
    "        other_posts.append(row)\n",
    "        \n",
    "print(len(ask_posts))\n",
    "print(len(show_posts))\n",
    "print(len(other_posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculating the Average Number of Comments for Ask HN and Show HN Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's determine if ask posts or show posts receive more comments on average.\n",
    "\n",
    "We now find the total number of comments for `Ask HN` and `Show HN` through the expression: `avg_num_comments = total_num_comments / length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of comments for Ask HN is: 14.038417431192661\n"
     ]
    }
   ],
   "source": [
    "total_ask_comments = 0\n",
    "for row in ask_posts:\n",
    "    comments = int(row[4])\n",
    "    total_ask_comments += comments\n",
    "\n",
    "avg_ask_comments = total_ask_comments / len(ask_posts)\n",
    "print(\"The average number of comments for Ask HN is:\", avg_ask_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of comments for Show HN is: 10.31669535283993\n"
     ]
    }
   ],
   "source": [
    "total_show_comments = 0\n",
    "for row in show_posts:\n",
    "    comments = int(row[4])\n",
    "    total_show_comments += comments\n",
    "\n",
    "avg_show_comments = total_show_comments / len(show_posts)\n",
    "print(\"The average number of comments for Show HN is:\", avg_show_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average number of comments per category:\n",
    "- Ask posts: 14.04\n",
    "- Show posts: 10.32\n",
    "\n",
    "Ask posts recieve an average of about four more comments than show posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate a percent difference in the number of posts and a percent difference in the average number of comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.37155963302752\n"
     ]
    }
   ],
   "source": [
    "# Percentage difference in the number of posts\n",
    "print(((len(ask_posts) - len(show_posts))/len(ask_posts))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.510980291006664\n"
     ]
    }
   ],
   "source": [
    "# Percentage difference in the avg number of comments\n",
    "print(((avg_ask_comments - avg_show_comments)/avg_ask_comments)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our summary measures above;\n",
    "\n",
    "- `Ask HN` posts are 33% more than `Show HN` posts.\n",
    "- `ASk HN` posts have 26% more comments on average than `Show HN` posts\n",
    "\n",
    "These findings answer our first question: `Which post receives more comments on average?`. This implies that `Ask HN` posts are more popular. There's more activity associated with these posts hence the site should prioritize them and provide additional resources in handling them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finding the Number of Ask Posts and Comments by Hour Created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll determine if ask posts created at a certain time are more likely to attract comments. We'll use the following steps to perform this analysis:\n",
    "\n",
    "\n",
    "1. Calculate the number of ask posts created in each hour of the day, along with the number of comments received.\n",
    "2. Calculate the average number of comments ask posts receive by hour created.\n",
    "\n",
    "We'll first work on the first step â€” calculating the number of ask posts and comments by hour created. We'll use the datetime module to work with the data in the created_at column. We'll use the [`datetime` module](https://docs.python.org/3/library/datetime.html) to work with the data in the created_at column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['8/16/2016 9:55', 6], ['11/22/2015 13:43', 29], ['5/2/2016 10:14', 1], ['8/2/2016 14:20', 3], ['10/15/2015 16:38', 17]]\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "result_list = []\n",
    "for row in ask_posts:\n",
    "    created_at = row[6]\n",
    "    num_comments = int(row[4])\n",
    "    result_list.append([created_at, num_comments])\n",
    "    \n",
    "print(result_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of lists containing the date and number of comments, we can split it further into two dictionaries:\n",
    "\n",
    "- `counts_by_hour`: contains the number of ask posts created during each hour of the day.\n",
    "- `comments_by_hour`: contains the corresponding number of comments ask posts created at each hour received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Posts per hour: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'09': 45,\n",
       " '13': 85,\n",
       " '10': 59,\n",
       " '14': 107,\n",
       " '16': 108,\n",
       " '23': 68,\n",
       " '12': 73,\n",
       " '17': 100,\n",
       " '15': 116,\n",
       " '21': 109,\n",
       " '20': 80,\n",
       " '02': 58,\n",
       " '18': 109,\n",
       " '03': 54,\n",
       " '05': 46,\n",
       " '19': 110,\n",
       " '01': 60,\n",
       " '22': 71,\n",
       " '08': 48,\n",
       " '04': 47,\n",
       " '00': 55,\n",
       " '06': 44,\n",
       " '07': 34,\n",
       " '11': 58}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Comments per hour: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'09': 251,\n",
       " '13': 1253,\n",
       " '10': 793,\n",
       " '14': 1416,\n",
       " '16': 1814,\n",
       " '23': 543,\n",
       " '12': 687,\n",
       " '17': 1146,\n",
       " '15': 4477,\n",
       " '21': 1745,\n",
       " '20': 1722,\n",
       " '02': 1381,\n",
       " '18': 1439,\n",
       " '03': 421,\n",
       " '05': 464,\n",
       " '19': 1188,\n",
       " '01': 683,\n",
       " '22': 479,\n",
       " '08': 492,\n",
       " '04': 337,\n",
       " '00': 447,\n",
       " '06': 397,\n",
       " '07': 267,\n",
       " '11': 641}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts_by_hour = {}\n",
    "comments_by_hour = {}\n",
    "\n",
    "for row in result_list:\n",
    "    date = row[0]\n",
    "    num_comments = row[1]\n",
    "    date = dt.datetime.strptime(date, \"%m/%d/%Y %H:%M\")\n",
    "    hour = date.strftime('%H')\n",
    "    \n",
    "    if hour not in counts_by_hour:\n",
    "        counts_by_hour[hour] = 1\n",
    "        comments_by_hour[hour] =  num_comments\n",
    "    else:\n",
    "        counts_by_hour[hour] += 1\n",
    "        comments_by_hour[hour] += num_comments\n",
    "        \n",
    "display(\"Posts per hour: \", counts_by_hour)\n",
    "display(\"Comments per hour: \", comments_by_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `hour` with the most `posts` was `3:00 PM` and the `hour` with the most `comments` was also `3:00 PM`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5. Calculating the Average Number of Comments for Ask HN Posts by Hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use these two dictionaries to calculate the average number of comments for posts created during each hour of the day. To do this, we'll create a list of lists containing the hours during which posts were created and the average number of comments those posts received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['09', 5.58],\n",
       " ['13', 14.74],\n",
       " ['10', 13.44],\n",
       " ['14', 13.23],\n",
       " ['16', 16.8],\n",
       " ['23', 7.99],\n",
       " ['12', 9.41],\n",
       " ['17', 11.46],\n",
       " ['15', 38.59],\n",
       " ['21', 16.01],\n",
       " ['20', 21.52],\n",
       " ['02', 23.81],\n",
       " ['18', 13.2],\n",
       " ['03', 7.8],\n",
       " ['05', 10.09],\n",
       " ['19', 10.8],\n",
       " ['01', 11.38],\n",
       " ['22', 6.75],\n",
       " ['08', 10.25],\n",
       " ['04', 7.17],\n",
       " ['00', 8.13],\n",
       " ['06', 9.02],\n",
       " ['07', 7.85],\n",
       " ['11', 11.05]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_by_hour = []\n",
    "\n",
    "for hour in comments_by_hour:\n",
    "    avg_by_hour.append([hour,round(comments_by_hour[hour] / counts_by_hour[hour],2)])\n",
    "    \n",
    "display(avg_by_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the highest average number of comments occurs at `3:00 PM`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sorting and Printing Values from a List of Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the previous cell, we calculated the average number of comments for posts created during each hour of the day, and stored the results in a list of lists named `avg_by_hour`. Although we now have the results we need, this format makes it difficult to identify the hours with the highest values. Let's finish by sorting the list of lists and printing the five highest values in a format that's easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5.58, '09'],\n",
       " [14.74, '13'],\n",
       " [13.44, '10'],\n",
       " [13.23, '14'],\n",
       " [16.8, '16'],\n",
       " [7.99, '23'],\n",
       " [9.41, '12'],\n",
       " [11.46, '17'],\n",
       " [38.59, '15'],\n",
       " [16.01, '21'],\n",
       " [21.52, '20'],\n",
       " [23.81, '02'],\n",
       " [13.2, '18'],\n",
       " [7.8, '03'],\n",
       " [10.09, '05'],\n",
       " [10.8, '19'],\n",
       " [11.38, '01'],\n",
       " [6.75, '22'],\n",
       " [10.25, '08'],\n",
       " [7.17, '04'],\n",
       " [8.13, '00'],\n",
       " [9.02, '06'],\n",
       " [7.85, '07'],\n",
       " [11.05, '11']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sort the values\n",
    "swap_avg_by_hour = []\n",
    "\n",
    "for row in avg_by_hour:\n",
    "    swap_avg_by_hour.append([row[1],row[0]])\n",
    "    \n",
    "display(swap_avg_by_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the [`sorted()` function](https://docs.python.org/3/library/functions.html#sorted) to sort `swap_avg_by_hour` in descending order. Since the first column of this list is the average number of comments, sorting the list will sort by the average number of comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Hours for Ask Posts Comments:\n",
      "15:00: 38.59 average comments per post\n",
      "02:00: 23.81 average comments per post\n",
      "20:00: 21.52 average comments per post\n",
      "16:00: 16.8 average comments per post\n",
      "21:00: 16.01 average comments per post\n",
      "13:00: 14.74 average comments per post\n",
      "10:00: 13.44 average comments per post\n",
      "14:00: 13.23 average comments per post\n",
      "18:00: 13.2 average comments per post\n",
      "17:00: 11.46 average comments per post\n",
      "01:00: 11.38 average comments per post\n",
      "11:00: 11.05 average comments per post\n",
      "19:00: 10.8 average comments per post\n",
      "08:00: 10.25 average comments per post\n",
      "05:00: 10.09 average comments per post\n",
      "12:00: 9.41 average comments per post\n",
      "06:00: 9.02 average comments per post\n",
      "00:00: 8.13 average comments per post\n",
      "23:00: 7.99 average comments per post\n",
      "07:00: 7.85 average comments per post\n",
      "03:00: 7.8 average comments per post\n",
      "04:00: 7.17 average comments per post\n",
      "22:00: 6.75 average comments per post\n",
      "09:00: 5.58 average comments per post\n"
     ]
    }
   ],
   "source": [
    "sorted_swap = sorted(swap_avg_by_hour, reverse = True)\n",
    "print(\"Top 5 Hours for Ask Posts Comments:\")\n",
    "for row in sorted_swap:\n",
    "    average = row[0]\n",
    "    hour = row[1]\n",
    "    hour = dt.datetime.strptime(hour,'%H')\n",
    "    hour = hour.strftime('%H:%M')\n",
    "    print('{}: {} average comments per post'.format(hour,average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary above, it is clear that the best time to create a post that will stand a chance of receiving more comments is between `15:00` and `16:00`. As the hacker news dataset is in the EST timezone, this corresponds to later in the afternoon between `3:00-4:00 PM EST`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions on the correlation between posting time and engagement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the best hour to post and get comments is from `3:00 - 4:00 PM EST`. \n",
    "\n",
    "Looking at the first several rows, we can see that the most engaging times, and therefore the times when moderators are most needed, are the early evening (`3-4 PM EST`), and at night (`02:00, 20:00). \n",
    "\n",
    "This makes sense since a lot of the participants might have full time jobs, hence that a lot of these happen either after work or right before finishing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
